#!/usr/bin/env python
# coding: utf-8

# # DeepDreaming with TensorFlow

# This notebook demonstrates a number of Convolutional Neural Network image generation techniques implemented with TensorFlow for fun and science:
# 
# - visualize individual feature channels and their combinations to explore the space of patterns learned by the neural network (see [GoogLeNet](http://storage.googleapis.com/deepdream/visualz/tensorflow_inception/index.html) and [VGG16](http://storage.googleapis.com/deepdream/visualz/vgg16/index.html) galleries)
# - embed TensorBoard graph visualizations into Jupyter notebooks
# - produce high-resolution images with tiled computation ([example](http://storage.googleapis.com/deepdream/pilatus_flowers.jpg))
# - use Laplacian Pyramid Gradient Normalization to produce smooth and colorful visuals at low cost
# - generate DeepDream-like images with TensorFlow (DogSlugs included)
# 
# 
# The network under examination is the [GoogLeNet architecture](http://arxiv.org/abs/1409.4842), trained to classify images into one of 1000 categories of the [ImageNet](http://image-net.org/) dataset. It consists of a set of layers that apply a sequence of transformations to the input image. The parameters of these transformations were determined during the training process by a variant of gradient descent algorithm. The internal image representations may seem obscure, but it is possible to visualize and interpret them. In this notebook we are going to present a few tricks that allow to make these visualizations both efficient to generate and even beautiful. Impatient readers can start with exploring the full galleries of images generated by the method described here for [GoogLeNet](http://storage.googleapis.com/deepdream/visualz/tensorflow_inception/index.html) and [VGG16](http://storage.googleapis.com/deepdream/visualz/vgg16/index.html) architectures.

# boilerplate code
from __future__ import print_function
import os
os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"   # see issue #152
os.environ["CUDA_VISIBLE_DEVICES"]='0'
from io import BytesIO
import numpy as np
from functools import partial
import PIL.Image
#from IPython.display import clear_output, Image, display, HTML

import tensorflow as tf
import pdb
import scipy.misc
#import matplotlib.pyplot as plt

import sys
import argparse

## Helper functions for TF Graph visualization

def strip_consts(graph_def, max_const_size=32):
    """Strip large constant values from graph_def."""
    strip_def = tf.GraphDef()
    for n0 in graph_def.node:
        n = strip_def.node.add() 
        n.MergeFrom(n0)
        if n.op == 'Const':
            tensor = n.attr['value'].tensor
            size = len(tensor.tensor_content)
            if size > max_const_size:
                tensor.tensor_content = tf.compat.as_bytes("<stripped %d bytes>"%size)
    return strip_def
  
def rename_nodes(graph_def, rename_func):
    res_def = tf.GraphDef()
    for n0 in graph_def.node:
        n = res_def.node.add() 
        n.MergeFrom(n0)
        n.name = rename_func(n.name)
        for i, s in enumerate(n.input):
            n.input[i] = rename_func(s) if s[0]!='^' else '^'+rename_func(s[1:])
    return res_def
  
#def show_graph(graph_def, max_const_size=32):
#    """Visualize TensorFlow graph."""
#    if hasattr(graph_def, 'as_graph_def'):
#        graph_def = graph_def.as_graph_def()
#    strip_def = strip_consts(graph_def, max_const_size=max_const_size)
#    code = """
#        <script>
#          function load() {{
#            document.getElementById("{id}").pbtxt = {data};
#          }}
#        </script>
#        <link rel="import" href="https://tensorboard.appspot.com/tf-graph-basic.build.html" onload=load()>
#        <div style="height:600px">
#          <tf-graph-basic id="{id}"></tf-graph-basic>
#        </div>
#    """.format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))
#  #
#    iframe = """
#        <iframe seamless style="width:800px;height:620px;border:0" srcdoc="{}"></iframe>
#    """.format(code.replace('"', '&quot;'))
#    display(HTML(iframe))

#def showarray(a, fmt='jpeg'):
def showarray(a, filename='test22.jpeg'):
    a = np.uint8(np.clip(a, 0, 1)*255)
    #f = BytesIO()
    #PIL.Image.fromarray(a).save(f, fmt)
    print(a.size)
    filename1 = os.path.join('/root/Wuhan_deepdreamed',filename)
    PIL.Image.fromarray(a).save(filename1)
    #pdb.set_trace()
    #display(Image(data=f.getvalue()))
    
def visstd(a, s=0.1):
    '''Normalize the image range for visualization'''
    return (a-a.mean())/max(a.std(), 1e-4)*s + 0.5

def T(layer, graph):
    '''Helper for getting layer output tensor'''
    return graph.get_tensor_by_name("import/%s:0"%layer)


def tffunc(*argtypes):
    '''Helper that transforms TF-graph generating function into a regular one.
    See "resize" function below.
    '''
    #pdb.set_trace()
    placeholders = list(map(tf.placeholder, argtypes))
    def wrap(f):
        out = f(*placeholders)
        def wrapper(*args, **kw):
            return out.eval(dict(zip(placeholders, args)), session=kw.get('session'))
        return wrapper
    return wrap

def resize(img, size):
    ## Helper function that uses TF to resize an image
    img = tf.expand_dims(img, 0)
    return tf.image.resize_bilinear(img, size)[0,:,:,:]
#resize = tffunc(np.float32, np.int32)(resize)


def lap_split(img):
    '''Split the image into lo and hi frequency components'''
    with tf.name_scope('split'):
        lo = tf.nn.conv2d(img, k5x5, [1,2,2,1], 'SAME')
        lo2 = tf.nn.conv2d_transpose(lo, k5x5*4, tf.shape(img), [1,2,2,1])
        hi = img-lo2
    return lo, hi

def lap_split_n(img, n):
    '''Build Laplacian pyramid with n splits'''
    levels = []
    for i in range(n):
        img, hi = lap_split(img)
        levels.append(hi)
    levels.append(img)
    return levels[::-1]

def lap_merge(levels):
    '''Merge Laplacian pyramid'''
    img = levels[0]
    for hi in levels[1:]:
        with tf.name_scope('merge'):
            img = tf.nn.conv2d_transpose(img, k5x5*4, tf.shape(hi), [1,2,2,1]) + hi
    return img

def normalize_std(img, eps=1e-10):
    '''Normalize image by making its standard deviation = 1.0'''
    with tf.name_scope('normalize'):
        std = tf.sqrt(tf.reduce_mean(tf.square(img)))
        return img/tf.maximum(std, eps)

def lap_normalize(img, scale_n=4):
    '''Perform the Laplacian pyramid normalization.'''
    img = tf.expand_dims(img,0)
    tlevels = lap_split_n(img, scale_n)
    tlevels = list(map(normalize_std, tlevels))
    out = lap_merge(tlevels)
    return out[0,:,:,:]


def calc_grad_tiled(sess, img, t_grad, t_input, tile_size=512):
    '''Compute the value of tensor t_grad over the image in a tiled way.
    Random shifts are applied to the image to blur tile boundaries over 
    multiple iterations.'''
    #
    sz = tile_size
    h, w = img.shape[:2]
    sx, sy = np.random.randint(sz, size=2)
    img_shift = np.roll(np.roll(img, sx, 1), sy, 0)
    grad = np.zeros_like(img)
    for y in range(0, max(h-sz//2, sz),sz):
        for x in range(0, max(w-sz//2, sz),sz):
            sub = img_shift[y:y+sz,x:x+sz]
            #print(sub.shape)
            g = sess.run(t_grad, {t_input:sub})
            grad[y:y+sz,x:x+sz] = g
    return np.roll(np.roll(grad, -sx, 1), -sy, 0)

def render_naive(sess, t_obj, t_input, img0, iter_n=520, step=1.0):
    t_score = tf.reduce_mean(t_obj) # defining the optimization objective
    t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!
    #
    img = img0.copy()
    for i in range(iter_n):
        g, score = sess.run([t_grad, t_score], {t_input:img})
        # normalizing the gradient, so the same step size should work 
        g /= g.std()+1e-8         # for different layers and networks
        img += g*step
        print(score, end = ' ')
    #clear_output()
    showarray(visstd(img),filename='image_dream_naive.jpg')

def render_multiscale(sess, t_obj, t_input, img0, iter_n=10, step=1.0, octave_n=3, octave_scale=1.4):
    t_score = tf.reduce_mean(t_obj) # defining the optimization objective
    t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!
    #
    img = img0.copy()
    for octave in range(octave_n):
        if octave>0:
            hw = np.float32(img.shape[:2])*octave_scale
            img = resize(img, np.int32(hw))
            #print(img.shape)

        for i in range(iter_n):
            g = calc_grad_tiled(sess, img, t_grad, t_input)
            # normalizing the gradient, so the same step size should work 
            g /= g.std()+1e-8         # for different layers and networks
            img += g*step
            print('.', end = ' ')
        #clear_output()
        showarray(visstd(img),filename='image_dream_multiscale.jpg')

def render_lapnorm(sess, t_obj, t_input, img0, visfunc=visstd, iter_n=10, step=1.0, octave_n=3, octave_scale=1.4, lap_n=4):
    t_score = tf.reduce_mean(t_obj) # defining the optimization objective
    t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!
    # build the laplacian normalization graph
    lap_norm_func = tffunc(np.float32)(partial(lap_normalize, scale_n=lap_n))
    #
    img = img0.copy()
    for octave in range(octave_n):
        if octave>0:
            hw = np.float32(img.shape[:2])*octave_scale
            img = resize(img, np.int32(hw))
        for i in range(iter_n):
            g = calc_grad_tiled(sess, img, t_grad, t_input)
            g = lap_norm_func(g)
            img += g*step
            print('.', end = ' ')
        #clear_output()
        showarray(visfunc(img))

def render_deepdream(sess, t_obj, t_input, img0, channel, iter_n=50, step=1.5, octave_n=4, octave_scale=1.4):
    # Now let's reproduce the [DeepDream algorithm](https://github.com/google/deepdream/blob/master/dream.ipynb) 
    # with TensorFlow. 
    #
    t_score = tf.reduce_mean(t_obj) # defining the optimization objective
    t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!
    #
    #pdb.set_trace()
    # split the image into a number of octaves
    img = img0
    octaves = []
    for i in range(octave_n-1):
        hw = img.shape[:2]
        #print(type(hw))
        #pdb.set_trace()
        lo = resize(img, np.int32(np.float32(hw)/octave_scale))
        hi = img-resize(lo, hw)
        img = lo
        octaves.append(hi)
        #
    # generate details octave by octave
    for octave in range(octave_n):
        if octave>0:
            hi = octaves[-octave]
            img = resize(img, hi.shape[:2])+hi
            #print(img.shape)
        for i in range(iter_n):
            g = calc_grad_tiled(sess, img, t_grad, t_input)
            img += g*(step / (np.abs(g).mean()+1e-7))
            #print('.',end = ' ')
        #clear_output()
        showarray(img/255.0,filename='image_dream_channel%d.jpg'%(channel))


###############################################
## MAIN FUNCTION
###############################################
#
parser = argparse.ArgumentParser()
#parser.add_argument("--gpu", type=str, help="specify which gpu to use default=[0]")
parser.add_argument("--image_file", type=str, default='noise', help="specifcy the image(s) to dream on, takes in a folder path, or image path or noise (to dream on a noise image) [default: noise]")
parser.add_argument("--dream_class", type=str, default='dome', help="specifcy the class you which to optimize within the input image, dependent upon vgg model used [default: dome]")
parser.add_argument("--dream_results_dir", type=str, default='dreaming_results', help="path location of results folder/where to save the dreamed image  [default: dreaming_results]")
parser.add_argument("--network_model", type=str, default='imagenet', help="specify the vgg model path to the location of the json arch file \n and \
                                                                        h5 weights file. If training a network, this specifies the location to save the model.\n Default is to load pretrained imagenet default=[imagenet]")
parser.add_argument("--image_h", type=int, default=224, help="specify image height default=[224]")
parser.add_argument("--image_w", type=int, default=224, help="specify image width default=[224]")
parser.add_argument("--image_c", type=int, default=3, help="specify image channels default=[3]")
parser.add_argument('--iterations', type=int, default=250, help='number of iterations, the higher the iterations \n the stronger the dreaming effect default=[250]')


args = parser.parse_args()
#
##############################################
# ## Loading and displaying the model graph ##
##############################################
# 
## download pretrained network/weights
#get_ipython().system('wget -nc https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip 
#                        && unzip -n inception5h.zip')
#
## name of .pb/protobuff file that holds model
#model_fn = '/root/inception5_weights/tensorflow_inception_graph.pb'
model_fn = args.network_model
#
## creating TensorFlow session and loading the model ##
graph = tf.Graph()
sess = tf.InteractiveSession(graph=graph)
with tf.gfile.FastGFile(model_fn, 'rb') as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())
t_input = tf.placeholder(np.float32, name='input') # define the input tensor
imagenet_mean = 117.0
t_preprocessed = tf.expand_dims(t_input-imagenet_mean, 0)
tf.import_graph_def(graph_def, {'input':t_preprocessed})
#pdb.set_trace()
resize = tffunc(np.float32, np.int32)(resize)
#
## To take a glimpse into the kinds of patterns that the network learned to recognize, we will try to generate images 
## that maximize the sum of activations of particular channel of a particular convolutional layer of the neural network. 
## The network we explore contains many convolutional layers, each of which outputs tens to hundreds of feature channels, 
## so we have plenty of patterns to explore.
layers = [op.name for op in graph.get_operations() if op.type=='Conv2D' and 'import/' in op.name]
all_layers = [fop.name for fop in graph.get_operations()]
#pdb.set_trace()
feature_nums = [int(graph.get_tensor_by_name(name+':0').get_shape()[-1]) for name in layers]
#

print('Number of layers', len(layers))
print('Total number of feature channels:', sum(feature_nums))
#pdb.set_trace()
#
## Visualizing the network graph. Be sure expand the "mixed" nodes to see their 
## internal structure. We are going to visualize "Conv2D" nodes.
#tmp_def = rename_nodes(graph_def, lambda s:"/".join(s.split('_',1)))
#show_graph(tmp_def)
#
#pdb.set_trace()

###################################
# ## Naive feature visualization ##
###################################
## Let's start with a naive way of visualizing these. Image-space gradient ascent!
#
## Picking some internal layer. Note that we use outputs before applying the ReLU nonlinearity
## to have non-zero gradients for features with negative initial activations.
layer = 'mixed4d_3x3_bottleneck_pre_relu'
#layer = 'softmax1_pre_activation'
channel = 19 # picking some feature channel to visualize
    
## start with a gray image with a little noise
#im_h = 720
#im_w = 1280
#img_noise = np.random.uniform(size=(im_h,im_w,3)) + 100.0 
#img0 = img_noise
#img0 = scipy.misc.imread('/root/image.jpg').astype(np.float32)
img0 = scipy.misc.imread(args.image_file).astype(np.float32)
#
render_naive_flag=False
if render_naive_flag:
    render_naive(sess, T(layer,graph)[:,:,:,channel], t_input, img0)   
    #render_naive(sess, T(layer,graph)[:,channel], t_input, img0)   
    #
    pdb.set_trace()

###################################
# ## Multiscale image generation ##
###################################
# 
## Looks like the network wants to show us something interesting! Let's help it. 
## We are going to apply gradient ascent on multiple scales. Details formed on smaller 
## scale will be upscaled and augmented with additional details on the next scale.
#
## With multiscale image generation it may be tempting to set the number of octaves to some high 
## value to produce wallpaper-sized images. Storing network activations and backprop values will quickly 
## run out of GPU memory in this case. There is a simple trick to avoid this: split the image into smaller tiles 
## and compute each tile gradient independently. 
## Applying random shifts to the image before every iteration helps avoid tile seams and improves the overall image quality.   
render_multiscale_flag=False
if render_multiscale_flag:
    resize = tffunc(np.float32, np.int32)(resize)
    #render_multiscale(sess, T(layer, graph)[:,:,:,channel], t_input, img0)
    render_multiscale(sess, T(layer, graph)[:,channel], t_input, img0)
    #
    pdb.set_trace()

################################################
# ## Laplacian Pyramid Gradient Normalization ##
################################################
## This looks better, but the resulting images mostly contain high frequencies. 
## Can we improve it? One way is to add a smoothness prior into the optimization objective. 
## This will effectively blur the image a little every iteration, suppressing the higher frequencies, 
## so that the lower frequencies can catch up. This will require more iterations to produce a nice image.
## Why don't we just boost lower frequencies of the gradient instead? One way to achieve this is through 
## the [Laplacian pyramid](https://en.wikipedia.org/wiki/Pyramid_%28image_processing%29#Laplacian_pyramid) decomposition.
## We call the resulting technique _Laplacian Pyramid Gradient Normalization_.
#k = np.float32([1,4,6,4,1])
#k = np.outer(k, k)
#k5x5 = k[:,:,None,None]/k.sum()*np.eye(3, dtype=np.float32)
#
## Showing the lap_normalize graph with TensorBoard
#lap_graph = tf.Graph()
#with lap_graph.as_default():
#    lap_in = tf.placeholder(np.float32, name='lap_in')
#    lap_out = lap_normalize(lap_in)
#show_graph(lap_graph)
#
#render_lapnorm(sess, T(layer, graph)[:,:,:,channel], t_input, img0)
#
#pdb.set_trace()
#
# ## Playing with feature visualizations 
## We got a nice smooth image using only 10 iterations per octave. 
## In case of running on GPU this takes just a few seconds. Let's try to visualize another channel from the same layer. 
## The network can generate wide diversity of patterns.
#render_lapnorm(sess, T(layer, graph)[:,:,:,65], t_input, img0) ## looks like buildings/walls
#
#
## Lower layers produce features of lower complexity.
#render_lapnorm(sess, T('mixed3b_1x1_pre_relu', graph)[:,:,:,101], t_input, img0)
#
#pdb.set_trace()
#
## There are many interesting things one may try. For example, optimizing a linear combination of features 
## often gives a "mixture" pattern.
#render_lapnorm(sess, T(layer, graph)[:,:,:,65]+T(layer, graph)[:,:,:,139], t_input, img0, octave_n=4)
#
#pdb.set_trace()
#

#################
# ## DeepDream ##
#################
## Let's load some image and populate it with DogSlugs (in case you've missed them).
#img0 = PIL.Image.open('pilatus800.jpg')
#data_root = 'kitti_withdepth'
#img_filepath = '/root/image.jpg'#os.path.join(data_root, '04_rgb.png')
#img0 = PIL.Image.open(img_filepath)
img0 = np.float32(img0)
#showarray(img0/255.0)   

#render_deepdream(sess, tf.square(T('mixed4c', graph)), t_input, img0) 

# Note that results can differ from the [Caffe](https://github.com/BVLC/caffe)'s implementation, 
# as we are using an independently trained network. Still, the network seems to like dogs and animal-like 
# features due to the nature of the ImageNet dataset.
# 
# Using an arbitrary optimization objective still works:
channel=143
layer = 'mixed4d_3x3_bottleneck_pre_relu'
for channel in range(143):
    render_deepdream(sess, T(layer, graph)[:,:,:,channel], t_input, img0, channel, iter_n=args.iterations) 


# Don't hesitate to use higher resolution inputs (also increase the number of octaves)! 
# Here is an [example](http://storage.googleapis.com/deepdream/pilatus_flowers.jpg) of running the flower dream 
# over the bigger image.    

# We hope that the visualization tricks described here may be helpful for analyzing representations 
# learned by neural networks or find their use in various artistic applications.


####################################################################################################################
#if __name__=='__main__':
#    ## run the main program for deep dream visualization
#    args=[]
#    main(args)
